{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generation of dinosaur names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Statistical generation\n",
    "\n",
    "The task:\n",
    "* Implement a function that collects ngrams from the corpus. The length of ngrams should be one of parameters.\n",
    "* Implement random sampling for picking the next letter based on the previous context.\n",
    "* Generate new dino species. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"^\"\n",
    "END = \"$\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dino_names = []\n",
    "with open(\"dinosaur-names.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        dino_names.append(START + line.strip() + END)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['^aachenosaurus$',\n",
       " '^aardonyx$',\n",
       " '^abdallahsaurus$',\n",
       " '^abelisaurus$',\n",
       " '^abrictosaurus$',\n",
       " '^abrosaurus$',\n",
       " '^abydosaurus$',\n",
       " '^acanthopholis$',\n",
       " '^achelousaurus$',\n",
       " '^acheroraptor$']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dino_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find most frequent ngrams just for fun :)\n",
    "\n",
    "def collect_ngrams(corpus, n):\n",
    "    ngrams = Counter()\n",
    "    for line in corpus:\n",
    "        line_split = list(line)\n",
    "        for i in range(len(line_split) - n + 1):\n",
    "            ngrams[tuple(line_split[i:i+n])] += 1\n",
    "    return ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('u', 'r', 'u', 's', '$'), 721),\n",
       " (('a', 'u', 'r', 'u', 's'), 719),\n",
       " (('s', 'a', 'u', 'r', 'u'), 717),\n",
       " (('o', 's', 'a', 'u', 'r'), 443),\n",
       " (('a', 's', 'a', 'u', 'r'), 105),\n",
       " (('n', 'o', 's', 'a', 'u'), 88),\n",
       " (('i', 's', 'a', 'u', 'r'), 72),\n",
       " (('c', 'e', 'r', 'a', 't'), 72),\n",
       " (('e', 'r', 'a', 't', 'o'), 72),\n",
       " (('r', 'o', 's', 'a', 'u'), 68)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams = collect_ngrams(dino_names, 5)\n",
    "ngrams.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute next letter frequencies based on the collected ngrams\n",
    "\n",
    "def collect_ngram_dist(corpus, n):\n",
    "    ngram_dist = defaultdict(dict)\n",
    "    for line in corpus:\n",
    "        line_split = list(line)\n",
    "        for i in range(len(line_split) - n + 1):\n",
    "            key = \"\".join(line_split[i:i+n-1])\n",
    "            val = line_split[i+n-1]\n",
    "            try:\n",
    "                ngram_dist[key][val] += 1\n",
    "            except:\n",
    "                ngram_dist[key][val] = 1\n",
    "    return ngram_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('^a',\n",
       " {'a': 2,\n",
       "  'b': 5,\n",
       "  'c': 9,\n",
       "  'd': 4,\n",
       "  'e': 6,\n",
       "  'f': 1,\n",
       "  'g': 7,\n",
       "  'h': 1,\n",
       "  'i': 1,\n",
       "  'j': 2,\n",
       "  'l': 25,\n",
       "  'm': 13,\n",
       "  'n': 26,\n",
       "  'o': 2,\n",
       "  'p': 4,\n",
       "  'q': 1,\n",
       "  'r': 23,\n",
       "  's': 8,\n",
       "  't': 6,\n",
       "  'u': 12,\n",
       "  'v': 7,\n",
       "  'z': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_freqs = collect_ngram_dist(dino_names, 3)\n",
    "list(ngram_freqs.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ð¡ompute next letter probabilities based on the collected ngrams\n",
    "\n",
    "def freqs_to_probs(ngram_freqs):\n",
    "    ngram_probs = defaultdict(dict)\n",
    "    for k, v in list(ngram_freqs.items()):\n",
    "        total = sum(v.values())\n",
    "        ngram_probs[k] = dict()\n",
    "        for i in v.keys():\n",
    "            ngram_probs[k][i] = v[i] / total\n",
    "    return ngram_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('^a',\n",
       " {'a': 0.012048192771084338,\n",
       "  'b': 0.030120481927710843,\n",
       "  'c': 0.05421686746987952,\n",
       "  'd': 0.024096385542168676,\n",
       "  'e': 0.03614457831325301,\n",
       "  'f': 0.006024096385542169,\n",
       "  'g': 0.04216867469879518,\n",
       "  'h': 0.006024096385542169,\n",
       "  'i': 0.006024096385542169,\n",
       "  'j': 0.012048192771084338,\n",
       "  'l': 0.15060240963855423,\n",
       "  'm': 0.0783132530120482,\n",
       "  'n': 0.1566265060240964,\n",
       "  'o': 0.012048192771084338,\n",
       "  'p': 0.024096385542168676,\n",
       "  'q': 0.006024096385542169,\n",
       "  'r': 0.13855421686746988,\n",
       "  's': 0.04819277108433735,\n",
       "  't': 0.03614457831325301,\n",
       "  'u': 0.07228915662650602,\n",
       "  'v': 0.04216867469879518,\n",
       "  'z': 0.006024096385542169})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_probs = freqs_to_probs(ngram_freqs)\n",
    "list(ngram_probs.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random element based on the collected ngrams and their probabilities\n",
    "\n",
    "def generate_letter(context, ngram_probs):\n",
    "    try:\n",
    "        options = ngram_probs[context]\n",
    "        rand_num = random.random()\n",
    "        total = 0\n",
    "        for k in options.keys():\n",
    "            total += options[k]\n",
    "            if total > rand_num:\n",
    "                return k\n",
    "    except:\n",
    "        return END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u', 'e', 'l', 'a', 'e', 'u', 'd', 'e', 'r', 'o']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[generate_letter('ab', ngram_probs) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new dino species\n",
    "\n",
    "def generate_dino(start, ngram_probs, n, end_sym):\n",
    "    next_letter = generate_letter(start, ngram_probs)\n",
    "    dino_name = start + next_letter\n",
    "    while next_letter != end_sym:\n",
    "        next_letter = generate_letter(dino_name[-1*(n-1):], ngram_probs)\n",
    "        dino_name += next_letter\n",
    "    return dino_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "europhosaurus\n",
      "nyorosaurus\n",
      "fosaurus\n",
      "wendromeus\n",
      "breuillosaurus\n",
      "velocosaurus\n",
      "padros\n",
      "unaashetris\n",
      "elorosaurus\n",
      "colong\n",
      "koreadnouchus\n",
      "oohkotasaurus\n",
      "spinosaurus\n",
      "itemodromeykosaurus\n",
      "shansutodraciliubanjaffia\n",
      "walker\n",
      "glisaurus\n",
      "clepisabeipiaosaurus\n",
      "microsaurus\n",
      "nanusor\n",
      "dromimus\n",
      "bellynashosaurus\n",
      "utabatitanyasus\n",
      "hippodromeus\n",
      "jobathus\n",
      "abrosaurus\n",
      "don\n",
      "airasaurus\n",
      "augus\n",
      "uintarchasillus\n"
     ]
    }
   ],
   "source": [
    "N = 4\n",
    "ngram_freqs = collect_ngram_dist(dino_names, N)\n",
    "ngram_probs = freqs_to_probs(ngram_freqs)\n",
    "possible_begs = [k for k in ngram_probs.keys() if k.startswith(START)]\n",
    "\n",
    "random.shuffle(possible_begs)\n",
    "\n",
    "for start in possible_begs[:30]:\n",
    "    print(generate_dino(start, ngram_probs, N, END)[1:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural generation\n",
    "\n",
    "The task:\n",
    "* Prepare the training data: input is the given context and output is the character that has to be predicted.\n",
    "* Map characters to integers and do one-hot encoding.\n",
    "* Train the model.\n",
    "* Call the model to predict the next character and implement random sampling.\n",
    "* Generate new dino species. :)\n",
    "\n",
    "References: https://machinelearningmastery.com/develop-character-based-neural-language-model-keras/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^aach e\n",
      "aache n\n",
      "achen o\n",
      "cheno s\n",
      "henos a\n",
      "enosa u\n",
      "nosau r\n",
      "osaur u\n",
      "sauru s\n",
      "aurus $\n",
      "^aard o\n",
      "aardo n\n",
      "ardon y\n",
      "rdony x\n",
      "donyx $\n",
      "^abda l\n",
      "abdal l\n",
      "bdall a\n",
      "dalla h\n",
      "allah s\n"
     ]
    }
   ],
   "source": [
    "# Prepare the training data\n",
    "\n",
    "N = 5\n",
    "X_str, y_str = [], []\n",
    "\n",
    "for name in dino_names:\n",
    "    for i in range(N, len(name)):\n",
    "        X_str.append(name[i-N:i])\n",
    "        y_str.append(name[i])\n",
    "\n",
    "for i, j in list(zip(X_str, y_str))[:20]:\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'$': 0, '^': 1, 'a': 2, 'b': 3, 'c': 4, 'd': 5, 'e': 6, 'f': 7, 'g': 8, 'h': 9, 'i': 10, 'j': 11, 'k': 12, 'l': 13, 'm': 14, 'n': 15, 'o': 16, 'p': 17, 'q': 18, 'r': 19, 's': 20, 't': 21, 'u': 22, 'v': 23, 'w': 24, 'x': 25, 'y': 26, 'z': 27}\n"
     ]
    }
   ],
   "source": [
    "# Map characters to integers\n",
    "\n",
    "chars = sorted(list(set([char for name in dino_names + [START, END] for char in name])))\n",
    "mapping = dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "print(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 2, 4, 9] [6]\n",
      "[2, 2, 4, 9, 6] [15]\n",
      "[2, 4, 9, 6, 15] [16]\n",
      "[4, 9, 6, 15, 16] [20]\n",
      "[9, 6, 15, 16, 20] [2]\n",
      "[6, 15, 16, 20, 2] [22]\n",
      "[15, 16, 20, 2, 22] [19]\n",
      "[16, 20, 2, 22, 19] [22]\n",
      "[20, 2, 22, 19, 22] [20]\n",
      "[2, 22, 19, 22, 20] [0]\n",
      "[1, 2, 2, 19, 5] [16]\n",
      "[2, 2, 19, 5, 16] [15]\n",
      "[2, 19, 5, 16, 15] [26]\n",
      "[19, 5, 16, 15, 26] [25]\n",
      "[5, 16, 15, 26, 25] [0]\n",
      "[1, 2, 3, 5, 2] [13]\n",
      "[2, 3, 5, 2, 13] [13]\n",
      "[3, 5, 2, 13, 13] [2]\n",
      "[5, 2, 13, 13, 2] [9]\n",
      "[2, 13, 13, 2, 9] [20]\n"
     ]
    }
   ],
   "source": [
    "X_int, y_int = [], []\n",
    "\n",
    "for i in X_str:\n",
    "    X_int.append([mapping[char] for char in i])\n",
    "for i in y_str:\n",
    "    y_int.append([mapping[char] for char in i])\n",
    "\n",
    "for i, j in list(zip(X_int, y_int))[:20]:\n",
    "    print(i, j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size is 28\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0.]]\n",
      "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Do one-hot encoding\n",
    "\n",
    "vocab_size = len(mapping)\n",
    "print(\"The vocab size is\", vocab_size)\n",
    "\n",
    "X = array([to_categorical(x, num_classes=vocab_size) for x in X_int])\n",
    "y = to_categorical(y_int, num_classes=vocab_size)\n",
    "\n",
    "print(X[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mromanyshyn/Library/Python/3.6/lib/python/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               51600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                2828      \n",
      "=================================================================\n",
      "Total params: 54,428\n",
      "Trainable params: 54,428\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "model = Sequential()\n",
    "# X is our input; adding one LSTM hidden layer with 100 memory cells\n",
    "model.add(LSTM(100, input_shape=(X.shape[1], X.shape[2])))\n",
    "# adding a fully connected output layer that outputs one vector\n",
    "# with a probability distribution across all characters\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/mromanyshyn/Library/Python/3.6/lib/python/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/100\n",
      " - 3s - loss: 2.1971 - accuracy: 0.3729\n",
      "Epoch 2/100\n",
      " - 4s - loss: 1.6802 - accuracy: 0.5192\n",
      "Epoch 3/100\n",
      " - 3s - loss: 1.5389 - accuracy: 0.5570\n",
      "Epoch 4/100\n",
      " - 3s - loss: 1.4559 - accuracy: 0.5856\n",
      "Epoch 5/100\n",
      " - 3s - loss: 1.3982 - accuracy: 0.5981\n",
      "Epoch 6/100\n",
      " - 3s - loss: 1.3498 - accuracy: 0.6119\n",
      "Epoch 7/100\n",
      " - 3s - loss: 1.3057 - accuracy: 0.6263\n",
      "Epoch 8/100\n",
      " - 3s - loss: 1.2682 - accuracy: 0.6356\n",
      "Epoch 9/100\n",
      " - 3s - loss: 1.2350 - accuracy: 0.6456\n",
      "Epoch 10/100\n",
      " - 3s - loss: 1.2017 - accuracy: 0.6585\n",
      "Epoch 11/100\n",
      " - 3s - loss: 1.1693 - accuracy: 0.6643\n",
      "Epoch 12/100\n",
      " - 3s - loss: 1.1402 - accuracy: 0.6744\n",
      "Epoch 13/100\n",
      " - 3s - loss: 1.1104 - accuracy: 0.6812\n",
      "Epoch 14/100\n",
      " - 3s - loss: 1.0817 - accuracy: 0.6879\n",
      "Epoch 15/100\n",
      " - 2s - loss: 1.0531 - accuracy: 0.6917\n",
      "Epoch 16/100\n",
      " - 2s - loss: 1.0248 - accuracy: 0.7038\n",
      "Epoch 17/100\n",
      " - 3s - loss: 0.9981 - accuracy: 0.7110\n",
      "Epoch 18/100\n",
      " - 2s - loss: 0.9694 - accuracy: 0.7157\n",
      "Epoch 19/100\n",
      " - 2s - loss: 0.9423 - accuracy: 0.7217\n",
      "Epoch 20/100\n",
      " - 2s - loss: 0.9152 - accuracy: 0.7324\n",
      "Epoch 21/100\n",
      " - 3s - loss: 0.8865 - accuracy: 0.7422\n",
      "Epoch 22/100\n",
      " - 3s - loss: 0.8611 - accuracy: 0.7446\n",
      "Epoch 23/100\n",
      " - 2s - loss: 0.8351 - accuracy: 0.7537\n",
      "Epoch 24/100\n",
      " - 3s - loss: 0.8090 - accuracy: 0.7618\n",
      "Epoch 25/100\n",
      " - 3s - loss: 0.7828 - accuracy: 0.7691\n",
      "Epoch 26/100\n",
      " - 4s - loss: 0.7559 - accuracy: 0.7770\n",
      "Epoch 27/100\n",
      " - 3s - loss: 0.7318 - accuracy: 0.7835\n",
      "Epoch 28/100\n",
      " - 4s - loss: 0.7074 - accuracy: 0.7887\n",
      "Epoch 29/100\n",
      " - 2s - loss: 0.6822 - accuracy: 0.7985\n",
      "Epoch 30/100\n",
      " - 4s - loss: 0.6576 - accuracy: 0.8040\n",
      "Epoch 31/100\n",
      " - 4s - loss: 0.6355 - accuracy: 0.8120\n",
      "Epoch 32/100\n",
      " - 5s - loss: 0.6104 - accuracy: 0.8198\n",
      "Epoch 33/100\n",
      " - 3s - loss: 0.5887 - accuracy: 0.8257\n",
      "Epoch 34/100\n",
      " - 3s - loss: 0.5679 - accuracy: 0.8342\n",
      "Epoch 35/100\n",
      " - 3s - loss: 0.5443 - accuracy: 0.8385\n",
      "Epoch 36/100\n",
      " - 3s - loss: 0.5257 - accuracy: 0.8485\n",
      "Epoch 37/100\n",
      " - 3s - loss: 0.5054 - accuracy: 0.8553\n",
      "Epoch 38/100\n",
      " - 3s - loss: 0.4864 - accuracy: 0.8581\n",
      "Epoch 39/100\n",
      " - 3s - loss: 0.4659 - accuracy: 0.8646\n",
      "Epoch 40/100\n",
      " - 3s - loss: 0.4498 - accuracy: 0.8717\n",
      "Epoch 41/100\n",
      " - 3s - loss: 0.4322 - accuracy: 0.8781\n",
      "Epoch 42/100\n",
      " - 3s - loss: 0.4168 - accuracy: 0.8797\n",
      "Epoch 43/100\n",
      " - 3s - loss: 0.3984 - accuracy: 0.8869\n",
      "Epoch 44/100\n",
      " - 3s - loss: 0.3861 - accuracy: 0.8905\n",
      "Epoch 45/100\n",
      " - 3s - loss: 0.3711 - accuracy: 0.8950\n",
      "Epoch 46/100\n",
      " - 3s - loss: 0.3606 - accuracy: 0.8969\n",
      "Epoch 47/100\n",
      " - 2s - loss: 0.3464 - accuracy: 0.9004\n",
      "Epoch 48/100\n",
      " - 3s - loss: 0.3344 - accuracy: 0.9046\n",
      "Epoch 49/100\n",
      " - 2s - loss: 0.3243 - accuracy: 0.9089\n",
      "Epoch 50/100\n",
      " - 3s - loss: 0.3130 - accuracy: 0.9109\n",
      "Epoch 51/100\n",
      " - 3s - loss: 0.3055 - accuracy: 0.9134\n",
      "Epoch 52/100\n",
      " - 5s - loss: 0.2963 - accuracy: 0.9150\n",
      "Epoch 53/100\n",
      " - 4s - loss: 0.2863 - accuracy: 0.9186\n",
      "Epoch 54/100\n",
      " - 4s - loss: 0.2797 - accuracy: 0.9199\n",
      "Epoch 55/100\n",
      " - 2s - loss: 0.2737 - accuracy: 0.9190\n",
      "Epoch 56/100\n",
      " - 3s - loss: 0.2655 - accuracy: 0.9222\n",
      "Epoch 57/100\n",
      " - 3s - loss: 0.2601 - accuracy: 0.9226\n",
      "Epoch 58/100\n",
      " - 3s - loss: 0.2555 - accuracy: 0.9233\n",
      "Epoch 59/100\n",
      " - 3s - loss: 0.2494 - accuracy: 0.9241\n",
      "Epoch 60/100\n",
      " - 3s - loss: 0.2435 - accuracy: 0.9263\n",
      "Epoch 61/100\n",
      " - 2s - loss: 0.2379 - accuracy: 0.9273\n",
      "Epoch 62/100\n",
      " - 3s - loss: 0.2369 - accuracy: 0.9255\n",
      "Epoch 63/100\n",
      " - 2s - loss: 0.2325 - accuracy: 0.9288\n",
      "Epoch 64/100\n",
      " - 3s - loss: 0.2291 - accuracy: 0.9281\n",
      "Epoch 65/100\n",
      " - 3s - loss: 0.2261 - accuracy: 0.9273\n",
      "Epoch 66/100\n",
      " - 3s - loss: 0.2224 - accuracy: 0.9273\n",
      "Epoch 67/100\n",
      " - 3s - loss: 0.2196 - accuracy: 0.9299\n",
      "Epoch 68/100\n",
      " - 3s - loss: 0.2169 - accuracy: 0.9279\n",
      "Epoch 69/100\n",
      " - 4s - loss: 0.2163 - accuracy: 0.9294\n",
      "Epoch 70/100\n",
      " - 3s - loss: 0.2105 - accuracy: 0.9303\n",
      "Epoch 71/100\n",
      " - 3s - loss: 0.2098 - accuracy: 0.9303\n",
      "Epoch 72/100\n",
      " - 3s - loss: 0.2094 - accuracy: 0.9292\n",
      "Epoch 73/100\n",
      " - 4s - loss: 0.2061 - accuracy: 0.9306\n",
      "Epoch 74/100\n",
      " - 4s - loss: 0.2035 - accuracy: 0.9316\n",
      "Epoch 75/100\n",
      " - 2s - loss: 0.2040 - accuracy: 0.9306\n",
      "Epoch 76/100\n",
      " - 3s - loss: 0.2016 - accuracy: 0.9322\n",
      "Epoch 77/100\n",
      " - 3s - loss: 0.1990 - accuracy: 0.9314\n",
      "Epoch 78/100\n",
      " - 3s - loss: 0.1967 - accuracy: 0.9322\n",
      "Epoch 79/100\n",
      " - 3s - loss: 0.1996 - accuracy: 0.9315\n",
      "Epoch 80/100\n",
      " - 3s - loss: 0.1931 - accuracy: 0.9320\n",
      "Epoch 81/100\n",
      " - 3s - loss: 0.1957 - accuracy: 0.9303\n",
      "Epoch 82/100\n",
      " - 2s - loss: 0.1937 - accuracy: 0.9303\n",
      "Epoch 83/100\n",
      " - 2s - loss: 0.1908 - accuracy: 0.9332\n",
      "Epoch 84/100\n",
      " - 3s - loss: 0.1922 - accuracy: 0.9306\n",
      "Epoch 85/100\n",
      " - 2s - loss: 0.1905 - accuracy: 0.9310\n",
      "Epoch 86/100\n",
      " - 3s - loss: 0.1899 - accuracy: 0.9305\n",
      "Epoch 87/100\n",
      " - 3s - loss: 0.1870 - accuracy: 0.9316\n",
      "Epoch 88/100\n",
      " - 4s - loss: 0.1875 - accuracy: 0.9313\n",
      "Epoch 89/100\n",
      " - 2s - loss: 0.1849 - accuracy: 0.9315\n",
      "Epoch 90/100\n",
      " - 3s - loss: 0.1853 - accuracy: 0.9322\n",
      "Epoch 91/100\n",
      " - 2s - loss: 0.1839 - accuracy: 0.9312\n",
      "Epoch 92/100\n",
      " - 2s - loss: 0.1834 - accuracy: 0.9333\n",
      "Epoch 93/100\n",
      " - 2s - loss: 0.1829 - accuracy: 0.9317\n",
      "Epoch 94/100\n",
      " - 2s - loss: 0.1820 - accuracy: 0.9314\n",
      "Epoch 95/100\n",
      " - 2s - loss: 0.1828 - accuracy: 0.9318\n",
      "Epoch 96/100\n",
      " - 3s - loss: 0.1812 - accuracy: 0.9303\n",
      "Epoch 97/100\n",
      " - 2s - loss: 0.1810 - accuracy: 0.9308\n",
      "Epoch 98/100\n",
      " - 2s - loss: 0.1779 - accuracy: 0.9319\n",
      "Epoch 99/100\n",
      " - 2s - loss: 0.1793 - accuracy: 0.9314\n",
      "Epoch 100/100\n",
      " - 2s - loss: 0.1791 - accuracy: 0.9303\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x13315de10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X, y, epochs=100, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save('model.h5')\n",
    "# dump(mapping, open('mapping.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '$', 1: '^', 2: 'a', 3: 'b', 4: 'c', 5: 'd', 6: 'e', 7: 'f', 8: 'g', 9: 'h', 10: 'i', 11: 'j', 12: 'k', 13: 'l', 14: 'm', 15: 'n', 16: 'o', 17: 'p', 18: 'q', 19: 'r', 20: 's', 21: 't', 22: 'u', 23: 'v', 24: 'w', 25: 'x', 26: 'y', 27: 'z'}\n"
     ]
    }
   ],
   "source": [
    "mapping_inv = dict((i, c) for c, i in list(mapping.items()))\n",
    "print(mapping_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_letter_nn(start_str, model, mapping, mapping_inv):\n",
    "    # encode the context\n",
    "    start_int = [mapping[char] for char in start_str]\n",
    "    #start_int = pad_sequences([start_int], maxlen=n, truncating='pre')\n",
    "    start = to_categorical(start_int, num_classes=len(mapping))\n",
    "    start = start.reshape(1, start.shape[0], start.shape[1])\n",
    "    # predict the next letter\n",
    "    options = model.predict(start, verbose=0)[0]\n",
    "    rand_num = random.random()\n",
    "    total = 0\n",
    "    for i in range(len(options)):\n",
    "        total += options[i]\n",
    "        if total > rand_num:\n",
    "            return mapping_inv[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_letter_nn(\"^acri\", model, mapping, mapping_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['s', 's', 't', 's', 'o', 's', 's', 's', 's', 's']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[generate_letter_nn('^acri', model, mapping, mapping_inv) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new dino species\n",
    "\n",
    "def generate_dino_nn(start, model, n, end_sym, mapping, mapping_inv):\n",
    "    next_letter = generate_letter_nn(start, model, mapping, mapping_inv)\n",
    "    dino_name = start + next_letter\n",
    "    while next_letter != end_sym:\n",
    "        next_letter = generate_letter_nn(dino_name[-1*n:], model, mapping, mapping_inv)\n",
    "        dino_name += next_letter\n",
    "    return dino_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'^acristatusaurus$'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_dino_nn(\"^acri\", model, 5, END, mapping, mapping_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rhinoceratops\n",
      "indosaurus\n",
      "ruehleia\n",
      "magnapaulicheirosaurus\n",
      "eoceratophoneus\n",
      "chingkankousaurus\n",
      "sinotyrannus\n",
      "velocipes\n",
      "daptasaurus\n",
      "saichania\n",
      "niobrarasaurus\n",
      "owenodon\n",
      "poekolophus\n",
      "pukyongosaurus\n",
      "deuterosaurus\n",
      "elachiosaurus\n",
      "griphosaurus\n",
      "boreonykus\n",
      "salimosaurus\n",
      "actiosaurus\n",
      "nemegtosaurus\n",
      "velocimamosaurus\n",
      "texasetes\n",
      "paleosaurus\n",
      "stegos\n",
      "balauiuscur\n",
      "acheroraptor\n",
      "triassolestes\n",
      "zuoyunlong\n",
      "ozraptor\n"
     ]
    }
   ],
   "source": [
    "possible_begs = [name[:5] for name in dino_names]\n",
    "random.shuffle(possible_begs)\n",
    "\n",
    "for start in possible_begs[:30]:\n",
    "    print(generate_dino_nn(start, model, 5, END, mapping, mapping_inv)[1:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
